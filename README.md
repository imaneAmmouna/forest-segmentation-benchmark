# Augmented Forest Segmentation Benchmark

This project benchmarks deep learning models, including **U-Net, Attention U-Net, DeepLabV3+, HRNet, and UNETR**, using **TensorFlow** to evaluate their performance in achieving precise segmentation of forest images. The main goal is to compare these models in terms of accuracy, robustness, and ability to capture fine details in complex forest environments.

- üìÑ Rapport du projet
Un aper√ßu d√©taill√© du travail r√©alis√© est disponible dans le document suivant :
üëâ [üìò Overview Of Some Segmentation Models (PDF)](Overview%20Of%20Some%20Segmentation%20Models.pdf)
## Overview

Ce projet se concentre sur la d√©tection et la segmentation des zones foresti√®res dans les images en utilisant des architectures de deep learning de pointe. Il exploite diff√©rents mod√®les de segmentation s√©mantique pour fournir une identification pr√©cise et efficace de la v√©g√©tation et des autres composantes foresti√®res.
L'objectif principal est de comparer la performance de diff√©rents mod√®les pour des t√¢ches de segmentation d'images environnementales et d'√©valuer leur efficacit√© √† l'aide de m√©triques telles que la Loss, le Dice Coefficient, l'Intersection over Union (IoU), le Recall et la Precision.
## Dataset

Le projet utilise le dataset **[Augmented Forest Segmentation](https://www.kaggle.com/datasets/quadeer15sh/augmented-forest-segmentation)** disponible sur Kaggle.  

### Contexte
La cat√©gorisation et la segmentation automatiques de la couverture foresti√®re sont essentielles pour le d√©veloppement durable et l'am√©nagement urbain. Les for√™ts jouent un r√¥le cl√© dans les services √©cosyst√©miques tels que la fibre, l‚Äô√©nergie, les loisirs, la biodiversit√©, le stockage et le flux de carbone, ainsi que la gestion de l‚Äôeau. Elles doivent √©galement √™tre identifi√©es avant toute activit√© industrielle n√©cessitant un travail sur le terrain.  
Les images satellites ou issues de la t√©l√©d√©tection peuvent √™tre utilis√©es pour identifier et segmenter les zones foresti√®res, permettant ainsi d‚Äôestimer la surface occup√©e par les for√™ts. Cette t√¢che est d√©finie comme un **probl√®me de segmentation binaire** pour d√©tecter les zones foresti√®res.

### Contenu
- Le dataset contient **5108 images a√©riennes** de dimension **256x256 pixels**.  
- Le fichier **meta_data.csv** fournit les informations concernant les images a√©riennes et leurs masques binaires correspondants.
## Mod√®les utilis√©s

Le projet inclut plusieurs mod√®les de segmentation d‚Äôimages bas√©s sur le deep learning, afin de comparer leurs performances pour la segmentation des zones foresti√®res :  

- **U-Net** : Architecture classique encodeur-d√©codeur avec des connexions de type "skip" pour r√©cup√©rer les d√©tails spatiaux perdus lors de l'encodage.  
- **Attention U-Net** : Extension du U-Net int√©grant des modules d‚Äôattention pour se concentrer sur les r√©gions pertinentes et am√©liorer la segmentation des objets complexes ou petits.  
- **DeepLabV3+** : Utilise le module Atrous Spatial Pyramid Pooling (ASPP) pour extraire des caract√©ristiques multi-√©chelles et segmenter efficacement des objets de tailles vari√©es.  
- **HRNet** : Maintient des repr√©sentations √† haute r√©solution tout au long du r√©seau pour pr√©server les d√©tails fins et obtenir une segmentation pr√©cise.  
- **UNETR** : Combine les Transformers avec une architecture de type U-Net pour capturer les d√©pendances globales et les d√©tails locaux, particuli√®rement utile pour les structures complexes et vari√©es.
## M√©triques d‚Äô√©valuation

Pour comparer les performances des mod√®les de segmentation, plusieurs m√©triques standards sont utilis√©es :  

- **Loss** : Mesure l‚Äôerreur globale entre les pr√©dictions et les masques r√©els.  
- **Dice Coefficient** : √âvalue la similarit√© entre la segmentation pr√©dite et la v√©rit√© terrain ; une valeur proche de 1 indique une tr√®s bonne correspondance.  
- **Intersection over Union (IoU)** : Mesure le chevauchement entre les pixels pr√©dits et les pixels r√©els de la classe d‚Äôint√©r√™t.  
- **Recall (Rappel)** : Proportion des pixels de la for√™t correctement identifi√©s parmi tous les pixels forestiers r√©els.  
- **Precision (Pr√©cision)** : Proportion des pixels pr√©dits comme for√™t qui sont effectivement des pixels forestiers.
## üìä R√©sultats

| Mod√®le          | IoU  | Dice Coefficient | Pr√©cision | Rappel | Loss  |
|-----------------|------|------------------|------------|---------|-------|
| **DeepLabV3+**      | **0.8093** | 0.8946 | **0.9208** | 0.8698 | **0.2973** |
| **HRNet**           | 0.8201 | 0.9012 | 0.8760 | **0.9278** | 0.3348 |
| **Attention-UNet**  | **0.8243** | **0.9037** | 0.8858 | 0.9223 | 0.3267 |
| **UNetR**           | 0.7539 | 0.8597 | 0.8351 | 0.8858 | 0.4178 |
| **UNet**            | 0.8114 | 0.8959 | 0.8334 | **0.9684** | 0.3356 |

---
Les r√©sultats mettent en √©vidence **Attention-UNet** et **DeepLabV3+** comme les mod√®les les plus performants pour la **segmentation de la couverture foresti√®re**.  

- **Attention-UNet** a obtenu le **meilleur Dice Coefficient (0.9037)** et un **rappel √©lev√© (0.9223)**, d√©montrant une excellente capacit√© √† capturer les zones foresti√®res dans leur globalit√©.  
- **DeepLabV3+**, avec un **Dice de 0.8946** et la **meilleure pr√©cision (0.9208)**, montre une excellente performance dans la **d√©limitation pr√©cise** des fronti√®res des zones foresti√®res.  

Ces deux mod√®les offrent un **√©quilibre optimal entre sensibilit√© de d√©tection et pr√©cision de segmentation**, ce qui en fait les **approches les plus efficaces** pour cette t√¢che de segmentation binaire.
Les performances obtenues sont prometteuses, mais les mod√®les peuvent encore √™tre am√©lior√©s par une optimisation des hyperparam√®tres, une meilleure augmentation des donn√©es ou l‚Äôint√©gration de techniques plus r√©centes.
## üå≤ Visualisation des r√©sultats

Les figures suivantes illustrent les **r√©sultats de segmentation obtenus par diff√©rents mod√®les** sur des √©chantillons du jeu de donn√©es *Augmented Forest Segmentation*.  
Chaque image montre la comparaison entre :  
- **L‚Äôimage d‚Äôentr√©e**  
- **Le masque r√©el (Ground Truth)**  
- **Le masque pr√©dit par le mod√®le**

### üîπ Attention U-Net
![Attention U-Net](exemple_resultats/attention_unet.JPG)

### üîπ U-Net
![U-Net](exemple_resultats/unet.JPG)

### üîπ UNETR
![UNETR](exemple_resultats/UNETR.JPG)

### üîπ HRNet
![HRNet](exemple_resultats/HRNet.JPG)

### üîπ DeepLabV3+
![DeepLabV3+](exemple_resultats/DeepLabV3.JPG)
---
Ces visualisations mettent en √©vidence les **diff√©rences de pr√©cision et de coh√©rence spatiale** entre les mod√®les.  
On observe notamment que :
- **Attention U-Net** et **DeepLabV3+** produisent des masques plus homog√®nes et mieux align√©s avec les contours r√©els des zones foresti√®res.  
- **HRNet** montre une bonne capacit√© de g√©n√©ralisation, mais avec une l√©g√®re tendance √† la surestimation.  
- **UNETR** reste moins pr√©cis, ce qui peut √™tre li√© √† la complexit√© du mod√®le et au besoin d‚Äôun plus grand nombre d‚Äô√©chantillons pour un entra√Ænement optimal.

